{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A debugger notebook which has the core code for research and hyperparameter tuning, without the streamlit chat interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cataluna84/anaconda3/envs/generative-ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import pinecone\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the dotenv file\n",
    "load_dotenv()\n",
    "\n",
    "openaiKey = os.getenv('OPENAI_KEY')\n",
    "pineconeKey = os.getenv('PINECONE_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg_template = SystemMessagePromptTemplate.from_template(\n",
    "    template=\"\"\"Answer the question as truthfully as possible using the provided context, \n",
    "and if the answer is not contained within the text below, say 'I don't know'\"\"\")\n",
    "\n",
    "human_msg_template = HumanMessagePromptTemplate.from_template(template=\"{input}\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [system_msg_template, MessagesPlaceholder(variable_name=\"history\"), human_msg_template])\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", openai_api_key=openaiKey)\n",
    "conversation = ConversationChain(\n",
    "    memory=ConversationBufferMemory(return_messages=True), \n",
    "    prompt=prompt_template, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = openaiKey\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "pineconeEnvironment = os.getenv('PINECONE_ENVIRONMENT')\n",
    "pineconeIndexName = os.getenv('PINECONE_INDEX_NAME')\n",
    "\n",
    "pinecone.init(api_key=pineconeKey, environment=pineconeEnvironment)\n",
    "index = pinecone.Index(pineconeIndexName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': '0dc22399-20b2-467c-a057-5cf28d615cb2',\n",
      "              'metadata': {'source': 'content/data/457 - Luna Dong.txt',\n",
      "                           'text': \"[00:13:43] Sam Charrington: It's an area \"\n",
      "                                   \"that we've been working at for a long \"\n",
      "                                   \"time. Are there synergies when you're \"\n",
      "                                   'looking at the problem in conjunction with '\n",
      "                                   'the knowledge graph problem, or do you '\n",
      "                                   'take off the shelf extraction techniques, '\n",
      "                                   'and then apply them in a vacuum and then '\n",
      "                                   'you have some bundle of structured '\n",
      "                                   'information that you then integrate into '\n",
      "                                   'your knowledge graph?'},\n",
      "              'score': 0.704101861,\n",
      "              'values': []},\n",
      "             {'id': 'c1f9ae01-2aab-4088-bd52-6eb4cac3c963',\n",
      "              'metadata': {'source': 'content/data/457 - Luna Dong.txt',\n",
      "                           'text': '[00:03:20] Sam Charrington: When you think '\n",
      "                                   'of knowledge graphs, and in particular, '\n",
      "                                   'product knowledge graphs, what are all of '\n",
      "                                   'the things that go into making a robust '\n",
      "                                   'knowledge graph?'},\n",
      "              'score': 0.703084707,\n",
      "              'values': []},\n",
      "             {'id': '69e09173-17bd-4af2-a7fa-8930ed87d5b1',\n",
      "              'metadata': {'source': 'content/data/457 - Luna Dong.txt',\n",
      "                           'text': '[00:03:00] Sam Charrington: Some layer on '\n",
      "                                   'top of all of the data to make it more '\n",
      "                                   'easily accessible, or some centralized '\n",
      "                                   'thing that sits on top of all of the '\n",
      "                                   \"information within an organization. It's \"\n",
      "                                   'interesting to think of a knowledge graph '\n",
      "                                   'as playing that role for many '\n",
      "                                   'organizations.\\n'\n",
      "                                   '\\n'\n",
      "                                   \"[00:03:19] Luna Dong: That's true.\"},\n",
      "              'score': 0.694598615,\n",
      "              'values': []}],\n",
      " 'namespace': ''}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Answer the question as truthfully as possible using the provided context, \n",
      "and if the answer is not contained within the text below, say 'I don't know'\n",
      "Human: Context:\n",
      "  Luna Dong: [00:13:43] Sam Charrington: It's an area that we've been working at for a long time. Are there synergies when you're looking at the problem in conjunction with the knowledge graph problem, or do you take off the shelf extraction techniques, and then apply them in a vacuum and then you have some bundle of structured information that you then integrate into your knowledge graph?\n",
      " Luna Dong: [00:03:20] Sam Charrington: When you think of knowledge graphs, and in particular, product knowledge graphs, what are all of the things that go into making a robust knowledge graph?\n",
      " Luna Dong: [00:03:00] Sam Charrington: Some layer on top of all of the data to make it more easily accessible, or some centralized thing that sits on top of all of the information within an organization. It's interesting to think of a knowledge graph as playing that role for many organizations.\n",
      "\n",
      "[00:03:19] Luna Dong: That's true. \n",
      "\n",
      " Query:\n",
      "What did they discuss about knowledge graphs?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The discussion about knowledge graphs revolved around their construction and integration. They talked about whether there are synergies when looking at the problem of knowledge graph construction in conjunction with other problems, or if it's more effective to use off-the-shelf extraction techniques and then integrate the resulting structured information into the knowledge graph. They also discussed the concept of a knowledge graph as a layer or centralized component that sits on top of all information within an organization, making it more easily accessible.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What did they discuss about knowledge graphs?\"  # \"What did the guests talk about AI Ethics?\"\n",
    "\n",
    "if query:\n",
    "    input_em = model.encode(query).tolist()\n",
    "    result = index.query(input_em, top_k=3, includeMetadata=True)\n",
    "    print(result)\n",
    "    context = (result['matches'][0]['metadata']['source'].split('-')[1]).split('.')[0] + \": \" + result['matches'][0]['metadata']['text'] \\\n",
    "        + \"\\n\" + (result['matches'][1]['metadata']['source'].split('-')[1]).split('.')[0] + \": \" + result['matches'][1]['metadata']['text'] \\\n",
    "        + \"\\n\" + (result['matches'][2]['metadata']['source'].split('-')[1]).split('.')[0] + \": \" + result['matches'][2]['metadata']['text']\n",
    "    # print(context)\n",
    "    response = conversation.predict(input=f\"Context:\\n {context} \\n\\n Query:\\n{query}\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
